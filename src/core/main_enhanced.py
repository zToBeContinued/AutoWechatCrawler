#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ä¸»æµç¨‹å°è£…ï¼ˆè‡ªåŠ¨åŒ–ç‰ˆæœ¬ï¼‰

èŒè´£ï¼š
1. åˆå§‹åŒ–æ—¥å¿—
2. è¯»å–é…ç½®ï¼ˆåŒ…å« Excel è·¯å¾„ç­‰ï¼‰
3. æµ‹è¯•æ•°æ®åº“è¿æ¥ï¼ˆå¯é€‰ï¼‰
4. å¯åŠ¨ AutomatedCrawler
"""

import os
import sys
import logging
from datetime import datetime
import traceback

from config import get_crawler_config, get_database_config
from src.database.database_manager import DatabaseManager
from src.core.automated_crawler import AutomatedCrawler


def setup_logging() -> logging.Logger:
    """åˆå§‹åŒ–æ—¥å¿—ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ä¸æ–‡ä»¶ï¼‰"""
    logger = logging.getLogger("wechat_spider_main")
    logger.setLevel(logging.INFO)

    # é¿å…é‡å¤æ·»åŠ  handler
    if logger.handlers:
        return logger

    log_dir = os.path.join(os.getcwd(), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    log_filename = os.path.join(log_dir, f"wechat_spider_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger


def main():
    """ä¸»ç¨‹åºå…¥å£ - å…¨è‡ªåŠ¨åŒ–çˆ¬å–"""
    logger = setup_logging()

    logger.info("=" * 80)
    logger.info("ğŸš€ å¾®ä¿¡å…¬ä¼—å·å…¨è‡ªåŠ¨çˆ¬å–æµç¨‹å¯åŠ¨ ğŸš€")
    logger.info("ç‰ˆæœ¬: v3.0 - å…¨è‡ªåŠ¨åŒ–ç‰ˆæœ¬")
    logger.info("æ‰§è¡Œæ—¶é—´: %s", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    logger.info("=" * 80)

    # è¯»å–çˆ¬è™«é…ç½®
    crawler_cfg = get_crawler_config()
    excel_file = crawler_cfg.get('excel_file', 'target_articles.xlsx')
    if not os.path.exists(excel_file):
        logger.error("âŒ æœªæ‰¾åˆ° Excel æ–‡ä»¶: %s", excel_file)
        logger.error("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•æ”¾ç½®ç›®æ ‡å…¬ä¼—å· Excel æ–‡ä»¶ (é»˜è®¤: target_articles.xlsx)")
        sys.exit(1)

    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    db_config = get_database_config()
    logger.info("ğŸ” æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    try:
        with DatabaseManager(**db_config) as db:
            count = db.get_articles_count()
            logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼å½“å‰å·²æœ‰ {count} ç¯‡æ–‡ç« ")
            save_to_db = True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        logger.warning("âš ï¸ å°†ä»…ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¸å†™å…¥æ•°æ®åº“")
        save_to_db = False

    try:
        logger.info("å¯åŠ¨å…¨è‡ªåŠ¨åŒ–çˆ¬å–æµç¨‹...")
        crawler = AutomatedCrawler(
            excel_path=excel_file,
            save_to_db=save_to_db,
            db_config=db_config,
            crawler_config=crawler_cfg,
        )
        success = crawler.run()
        if success:
            logger.info("âœ… çˆ¬å–æµç¨‹å®Œæˆï¼Œç¨‹åºæ­£å¸¸ç»“æŸ")
            sys.exit(0)
        else:
            logger.error("âŒ çˆ¬å–æµç¨‹å¤±è´¥")
            sys.exit(1)
    except ImportError as e:
        logger.error("âŒ ä¾èµ–åº“ç¼ºå¤±: %s", e)
        logger.error("è¯·å…ˆå®‰è£…ä¾èµ–: pip install -r requirements.txt")
        sys.exit(1)
    except Exception as e:
        logger.error("âŒ ä¸»æµç¨‹å¼‚å¸¸: %s", e)
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == '__main__':
    main()