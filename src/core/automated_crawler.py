# automated_crawler.py
"""
å…¨æ–°çš„å…¨è‡ªåŠ¨åŒ–çˆ¬è™«æ§åˆ¶å™¨ - æ”¯æŒå¤šå…¬ä¼—å·
"""
import logging
import time
import os
import json
import pandas as pd

from src.proxy.read_cookie import ReadCookie
from src.crawler.batch_readnum_spider import BatchReadnumSpider
from src.ui.excel_auto_crawler import ExcelAutoCrawler
from src.database.database_manager import DatabaseManager
from src.database.database_config import get_database_config
from src.ui.wechat_browser_automation import WeChatBrowserAutomation, UI_AUTOMATION_AVAILABLE
from config.config_manager import get_crawler_config
from src.core.backfill_manager import BackfillManager, BackfillStageInfo

class AutomatedCrawler:
    """
    åè°ƒæ•´ä¸ªè‡ªåŠ¨åŒ–æµç¨‹çš„æ§åˆ¶å™¨ - æ”¯æŒå¤šå…¬ä¼—å·:
    1. ä»Excelè¯»å–æ‰€æœ‰å…¬ä¼—å·é“¾æ¥
    2. å¯¹æ¯ä¸ªå…¬ä¼—å·æ‰§è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹:
       - å¯åŠ¨ mitmproxy æŠ“å–å™¨ (ä¼šè‡ªåŠ¨è®¾ç½®ä»£ç†)
       - è¿è¡Œ UI è‡ªåŠ¨åŒ–æ‰“å¼€å¾®ä¿¡æ–‡ç« ä»¥è§¦å‘æŠ“å–
       - ç­‰å¾…å¹¶éªŒè¯ Cookie æ˜¯å¦æˆåŠŸæŠ“å–
       - åœæ­¢ mitmproxy æŠ“å–å™¨ (ä¼šè‡ªåŠ¨å…³é—­ä»£ç†)
       - ä½¿ç”¨è·å–åˆ°çš„ Cookie è¿è¡Œæ‰¹é‡çˆ¬è™«
    3. æ±‡æ€»æ‰€æœ‰å…¬ä¼—å·çš„æŠ“å–ç»“æœ
    """
    def __init__(self, excel_path="target_articles.xlsx", save_to_db=True, db_config=None, crawler_config=None):
        self.logger = logging.getLogger()
        # è‹¥æœªæ˜¾å¼ä¼ å…¥ excel_path åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ excel_file
        cfg_excel = (crawler_config or get_crawler_config()).get('excel_file', 'target_articles.xlsx')
        self.excel_path = excel_path if excel_path != "target_articles.xlsx" else cfg_excel
        # é…ç½®
        self.crawler_config = crawler_config or get_crawler_config()
        self.cookie_wait_timeout = self.crawler_config.get('cookie_wait_timeout', 120)
        self.account_delay = self.crawler_config.get('account_delay', 15)
        self.days_back = self.crawler_config.get('days_back', 90)
        self.max_pages = self.crawler_config.get('max_pages', 200)
        self.articles_per_page = self.crawler_config.get('articles_per_page', 5)
        # æ•°æ®åº“
        self.save_to_db = save_to_db
        self.db_config = db_config or get_database_config()
        if self.save_to_db:
            try:
                with DatabaseManager(**self.db_config) as db:
                    count = db.get_articles_count()
                    self.logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼å½“å‰æœ‰ {count} ç¯‡æ–‡ç« ")
            except Exception as e:
                self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                self.logger.warning("âš ï¸ å°†åªä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“")
                self.save_to_db = False

    def _get_all_target_urls_from_excel(self) -> list:
        """
        ä»Excelæ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰æœ‰æ•ˆçš„å…¬ä¼—å·é“¾æ¥
        :return: åŒ…å«æ‰€æœ‰æœ‰æ•ˆé“¾æ¥å’Œå…¬ä¼—å·åç§°çš„åˆ—è¡¨
        """
        self.logger.info(f"æ­£åœ¨ä» {self.excel_path} è¯»å–æ‰€æœ‰ç›®æ ‡URL...")
        if not os.path.exists(self.excel_path):
            self.logger.error(f"Excelæ–‡ä»¶æœªæ‰¾åˆ°: {self.excel_path}")
            return []

        try:
            df = pd.read_excel(self.excel_path)
            url_column = 'æ–‡ç« é“¾æ¥' if 'æ–‡ç« é“¾æ¥' in df.columns else 'url'
            name_column = 'å…¬ä¼—å·åç§°' if 'å…¬ä¼—å·åç§°' in df.columns else 'name'

            if url_column not in df.columns:
                self.logger.error("Excelä¸­æœªæ‰¾åˆ° 'æ–‡ç« é“¾æ¥' æˆ– 'url' åˆ—ã€‚")
                return []

            valid_targets = []
            for index, row in df.iterrows():
                url = row[url_column]
                name = row.get(name_column, f"å…¬ä¼—å·_{index+1}") if name_column in df.columns else f"å…¬ä¼—å·_{index+1}"

                if pd.notna(url) and 'mp.weixin.qq.com' in str(url):
                    valid_targets.append({
                        'name': str(name),
                        'url': str(url),
                        'index': index + 1
                    })
                    self.logger.info(f"æ‰¾åˆ°æœ‰æ•ˆç›®æ ‡ {index+1}: {name} - {str(url)[:50]}...")

            self.logger.info(f"å…±æ‰¾åˆ° {len(valid_targets)} ä¸ªæœ‰æ•ˆçš„å…¬ä¼—å·ç›®æ ‡")
            return valid_targets

        except Exception as e:
            self.logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤šå…¬ä¼—å·è‡ªåŠ¨åŒ–æµç¨‹"""
        self.logger.info("="*80)
        self.logger.info("ğŸš€ å¤šå…¬ä¼—å·å…¨æ–°è‡ªåŠ¨åŒ–æµç¨‹å¯åŠ¨ ğŸš€")
        self.logger.info("="*80)

        # åˆ†æ®µå›å¡«é˜¶æ®µå†³ç­–
        backfill_mgr = BackfillManager(self.crawler_config)
        stage_info = backfill_mgr.decide_stage()
        lower_dt = upper_dt = None
        stage_label = None
        adaptive_est_pages = 0
        if stage_info:
            lower_dt, upper_dt = backfill_mgr.compute_bounds(stage_info)
            stage_label = f"{stage_info.index}/{stage_info.total} {stage_info.lower_days}->{stage_info.upper_days}d"
            self.logger.info(f"ğŸ§© æ£€æµ‹åˆ°åˆ†æ®µå›å¡«é˜¶æ®µ: {stage_label} æ—¶é—´çª—å£ {lower_dt} -> {upper_dt}")
            adaptive_est_pages = backfill_mgr.decide_max_pages("__GLOBAL__", stage_info, self.articles_per_page)
            if adaptive_est_pages:
                self.logger.info(f"ğŸ§  è‡ªé€‚åº”ä¼°ç®— max_pages = {adaptive_est_pages} (å…¨å±€é…ç½® {self.max_pages})")
        else:
            self.logger.info("ğŸ§© åˆ†æ®µå›å¡«æœªå¯ç”¨æˆ–å·²å®Œæˆï¼Œä½¿ç”¨å¸¸è§„ days_back çª—å£")
            if self.crawler_config.get('adaptive_max_pages_enabled'):
                try:
                    synthetic_stage = BackfillStageInfo(0, self.days_back, 1, 1)
                    adaptive_est_pages = backfill_mgr.decide_max_pages("__GLOBAL__", synthetic_stage, self.articles_per_page)
                    if adaptive_est_pages:
                        self.logger.info(f"ğŸ§  éåˆ†æ®µæ¨¡å¼è‡ªé€‚åº”ä¼°ç®— max_pages = {adaptive_est_pages} (å…¨å±€é…ç½® {self.max_pages})")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ éåˆ†æ®µè‡ªé€‚åº”ä¼°ç®—å¤±è´¥: {e}")

        # è·å–æ‰€æœ‰ç›®æ ‡å…¬ä¼—å·
        all_targets = self._get_all_target_urls_from_excel()
        if not all_targets:
            self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å…¬ä¼—å·é“¾æ¥ï¼Œæµç¨‹ä¸­æ­¢ã€‚")
            return False

        self.logger.info(f"ğŸ“‹ å…±æ‰¾åˆ° {len(all_targets)} ä¸ªå…¬ä¼—å·ï¼Œå¼€å§‹é€ä¸ªå¤„ç†...")

        # ç”¨äºå­˜å‚¨æ‰€æœ‰å…¬ä¼—å·çš„æŠ“å–ç»“æœ
        all_results = []
        successful_count = 0
        failed_count = 0

        try:
            for i, target in enumerate(all_targets, 1):
                self.logger.info("="*60)
                self.logger.info(f"ğŸ“ å¤„ç†ç¬¬ {i}/{len(all_targets)} ä¸ªå…¬ä¼—å·: {target['name']}")
                self.logger.info("="*60)

                # ä¸ºæ¯ä¸ªå…¬ä¼—å·åˆ›å»ºç‹¬ç«‹çš„CookieæŠ“å–å™¨
                cookie_reader = None
                try:
                    # æ­¥éª¤1: ä¸ºæ¯ä¸ªå…¬ä¼—å·åˆ›å»ºç‹¬ç«‹çš„CookieæŠ“å–å™¨
                    self.logger.info(f"[æ­¥éª¤ 1/5] ä¸º '{target['name']}' åˆ›å»ºç‹¬ç«‹çš„ Cookie æŠ“å–å™¨...")
                    cookie_reader = ReadCookie()  # æ¯ä¸ªå…¬ä¼—å·ç‹¬ç«‹åˆ›å»ºï¼Œä¼šåˆ é™¤æ—§æ–‡ä»¶

                    if not cookie_reader.start_cookie_extractor():
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' Cookie æŠ“å–å™¨å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                        failed_count += 1
                        continue
                    self.logger.info("âœ… Cookie æŠ“å–å™¨å·²åœ¨åå°è¿è¡Œã€‚")

                    # æ­¥éª¤2: è¿è¡Œ UI è‡ªåŠ¨åŒ–è§¦å‘æŠ“å–
                    self.logger.info(f"[æ­¥éª¤ 2/5] ä¸º '{target['name']}' å¯åŠ¨ UI è‡ªåŠ¨åŒ–...")
                    try:
                        ui_crawler = ExcelAutoCrawler()
                        # ç›´æ¥ä¼ é€’å½“å‰å…¬ä¼—å·çš„URLï¼Œå¹¶ä¼ é€’cookie_readerä»¥å¯ç”¨æ™ºèƒ½åˆ·æ–°åœæ­¢
                        success = ui_crawler.automation.send_and_open_latest_link(target['url'], cookie_reader=cookie_reader)
                        if not success:
                            self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' UI è‡ªåŠ¨åŒ–è§¦å‘å¤±è´¥ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                            cookie_reader.stop_cookie_extractor()
                            failed_count += 1
                            continue
                    except Exception as e:
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' UI è‡ªåŠ¨åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                        cookie_reader.stop_cookie_extractor()
                        failed_count += 1
                        continue
                    self.logger.info("âœ… UI è‡ªåŠ¨åŒ–å·²æˆåŠŸè§¦å‘é“¾æ¥æ‰“å¼€ã€‚")

                    # æ­¥éª¤3: ç­‰å¾…å¹¶éªŒè¯ Cookie
                    self.logger.info(f"[æ­¥éª¤ 3/5] ç­‰å¾… '{target['name']}' çš„ Cookie æ•°æ®...")
                    if not cookie_reader.wait_for_new_cookie(timeout=self.cookie_wait_timeout):
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' ç­‰å¾… Cookie è¶…æ—¶ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                        cookie_reader.stop_cookie_extractor()
                        failed_count += 1
                        continue

                    # éªŒè¯cookieæ˜¯å¦æœ‰æ•ˆ
                    auth_info = cookie_reader.get_latest_cookies()
                    if not auth_info:
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' Cookie è§£æå¤±è´¥")
                        self.logger.error("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                        self.logger.error("   1. mitmproxy æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°å¾®ä¿¡è¯·æ±‚")
                        self.logger.error("   2. å¾®ä¿¡å†…ç½®æµè§ˆå™¨æ²¡æœ‰æ­£ç¡®æ‰“å¼€é“¾æ¥")
                        self.logger.error("   3. ç½‘ç»œè¿æ¥é—®é¢˜æˆ–ä»£ç†è®¾ç½®é—®é¢˜")
                        self.logger.error("ğŸ’¡ å»ºè®®:")
                        self.logger.error("   1. æ£€æŸ¥å¾®ä¿¡æ˜¯å¦æ­£å¸¸æ‰“å¼€äº†æ–‡ç« é“¾æ¥")
                        self.logger.error("   2. æ‰‹åŠ¨åœ¨å¾®ä¿¡ä¸­åˆ·æ–°æ–‡ç« é¡µé¢")
                        self.logger.error("   3. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
                        cookie_reader.stop_cookie_extractor()
                        failed_count += 1
                        continue
                    self.logger.info("âœ… æˆåŠŸè·å–å¹¶éªŒè¯äº†æ–°çš„ Cookieã€‚")

                    # æ­¥éª¤4: åœæ­¢ mitmproxy æŠ“å–å™¨
                    self.logger.info(f"[æ­¥éª¤ 4/5] åœæ­¢ '{target['name']}' çš„ Cookie æŠ“å–å™¨...")
                    cookie_reader.stop_cookie_extractor()
                    time.sleep(3)  # ç­‰å¾…ä»£ç†å®Œå…¨å…³é—­
                    self.logger.info("âœ… Cookie æŠ“å–å™¨å·²åœæ­¢ï¼Œç³»ç»Ÿä»£ç†å·²æ¢å¤ã€‚")

                    # æ­¥éª¤5: è¿è¡Œæ‰¹é‡çˆ¬è™«ï¼ˆå¸¦Cookieé‡æ–°æŠ“å–æœºåˆ¶ï¼‰
                    self.logger.info(f"[æ­¥éª¤ 5/5] å¼€å§‹çˆ¬å– '{target['name']}' çš„æ–‡ç« ...")

                    max_attempts = 2  # æœ€å¤šå°è¯•2æ¬¡ï¼ˆç¬¬ä¸€æ¬¡å¤±è´¥åé‡æ–°æŠ“å–Cookieå†è¯•ä¸€æ¬¡ï¼‰
                    batch_spider = None

                    for attempt in range(max_attempts):
                        try:
                            self.logger.info(f"ğŸ”„ ç¬¬ {attempt + 1}/{max_attempts} æ¬¡å°è¯•çˆ¬å–...")
                            batch_spider = BatchReadnumSpider(
                                auth_info=auth_info,
                                save_to_db=self.save_to_db,
                                db_config=self.db_config,
                                unit_name=target['name']
                            )

                            # å…ˆéªŒè¯Cookie
                            if not batch_spider.validate_cookie():
                                if attempt < max_attempts - 1:
                                    self.logger.warning("âš ï¸ CookieéªŒè¯å¤±è´¥ï¼ˆret=-3ï¼‰ï¼Œå‡†å¤‡ä»…åˆ·æ–°æ–‡ç« é¡µé¢ä»¥é‡æ–°æŠ“åŒ…...")

                                    # é‡æ–°æŠ“å–Cookieï¼ˆä»…å¯åŠ¨æŠ“å–å™¨ï¼Œä¸é‡å¤ç²˜è´´ç‚¹å‡»ï¼‰
                                    self.logger.info("ğŸ”„ é‡æ–°å¯åŠ¨CookieæŠ“å–å™¨...")
                                    fresh_cookie_reader = ReadCookie()
                                    if not fresh_cookie_reader.start_cookie_extractor():
                                        self.logger.error("âŒ é‡æ–°å¯åŠ¨CookieæŠ“å–å™¨å¤±è´¥")
                                        break

                                    # ä»…åˆ·æ–°å½“å‰æ–‡ç« é¡µé¢
                                    try:
                                        if not UI_AUTOMATION_AVAILABLE:
                                            self.logger.error("âŒ UIè‡ªåŠ¨åŒ–ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œåˆ·æ–°")
                                        else:
                                            self.logger.info("ğŸ” ä¸é‡æ–°ç²˜è´´é“¾æ¥ï¼Œç›´æ¥åˆ·æ–°å·²æ‰“å¼€çš„æ–‡ç« é¡µé¢ä»¥è§¦å‘æ–°è¯·æ±‚â€¦")
                                            refresher = WeChatBrowserAutomation()
                                            # åˆ·æ–°æ¬¡æ•°é€‚å½“å¢åŠ ï¼Œæé«˜è§¦å‘æ¦‚ç‡
                                            refresher.auto_refresh_browser(refresh_count=self.crawler_config.get('refresh_count', 3),
                                                                           refresh_delay=self.crawler_config.get('refresh_delay', 3.0),
                                                                           cookie_reader=fresh_cookie_reader)
                                    except Exception as e:
                                        self.logger.warning(f"åˆ·æ–°æ–‡ç« é¡µé¢æ—¶å‡ºé”™: {e}")

                                    # ç­‰å¾…æ–°Cookie
                                    if not fresh_cookie_reader.wait_for_new_cookie(timeout=self.cookie_wait_timeout):
                                        self.logger.error("âŒ é‡æ–°ç­‰å¾…Cookieè¶…æ—¶")
                                        fresh_cookie_reader.stop_cookie_extractor()
                                        break

                                    # è·å–æ–°çš„è®¤è¯ä¿¡æ¯
                                    auth_info = fresh_cookie_reader.get_latest_cookies()
                                    fresh_cookie_reader.stop_cookie_extractor()
                                    time.sleep(3)

                                    if not auth_info:
                                        self.logger.error("âŒ é‡æ–°è·å–Cookieå¤±è´¥")
                                        break

                                    self.logger.info("âœ… æˆåŠŸé€šè¿‡åˆ·æ–°é‡æ–°è·å–Cookieï¼Œç»§ç»­å°è¯•...")
                                    continue
                                else:
                                    self.logger.error("âŒ å¤šæ¬¡å°è¯•åCookieä»ç„¶æ— æ•ˆ")
                                    break

                            # Cookieæœ‰æ•ˆï¼Œå¼€å§‹æ­£å¼çˆ¬å–
                            self.logger.info("âœ… CookieéªŒè¯æˆåŠŸï¼Œå¼€å§‹æ­£å¼çˆ¬å–...")
                            # é’ˆå¯¹è¯¥è´¦å·çš„è‡ªé€‚åº”ä¼°ç®—ï¼ˆä¼˜å…ˆä½¿ç”¨é˜¶æ®µ/è™šæ‹Ÿé˜¶æ®µçª—å£å¤©æ•°ï¼‰
                            per_account_est = 0
                            if self.crawler_config.get('adaptive_max_pages_enabled'):
                                try:
                                    # å¤ç”¨ stage_info æˆ–æ„é€ è™šæ‹Ÿé˜¶æ®µ
                                    if stage_info:
                                        acct_stage = stage_info
                                    else:
                                        from src.core.backfill_manager import BackfillStageInfo
                                        acct_stage = BackfillStageInfo(0, self.days_back, 1, 1)
                                    per_account_est = BackfillManager(self.crawler_config).decide_max_pages(target['name'], acct_stage, self.articles_per_page)
                                    if per_account_est:
                                        self.logger.info(f"ğŸ§  è´¦å· {target['name']} è‡ªé€‚åº”ä¼°ç®— max_pages = {per_account_est}")
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ è´¦å·çº§è‡ªé€‚åº”ä¼°ç®—å¤±è´¥: {e}")
                            effective_max_pages = per_account_est or adaptive_est_pages or self.max_pages
                            batch_spider.batch_crawl_readnum(
                                max_pages=effective_max_pages,
                                articles_per_page=self.articles_per_page,
                                days_back=self.days_back,
                                lower_bound_dt=lower_dt,
                                upper_bound_dt=upper_dt,
                                stage_label=stage_label
                            )
                            # çˆ¬å–åæ›´æ–°è‡ªé€‚åº”ç»Ÿè®¡
                            try:
                                if self.crawler_config.get('adaptive_max_pages_enabled') and hasattr(batch_spider, 'crawl_stats'):
                                    stats = batch_spider.crawl_stats
                                    if stage_info:
                                        update_stage = stage_info
                                    else:
                                        from src.core.backfill_manager import BackfillStageInfo
                                        update_stage = BackfillStageInfo(0, self.days_back, 1, 1)
                                    backfill_mgr.update_account_stats(
                                        account=target['name'],
                                        stage=update_stage,
                                        used_pages=stats.get('used_pages', 0),
                                        effective_articles=stats.get('effective_articles', 0),
                                        last_page_effective=stats.get('last_page_effective', 0),
                                        last_page_total=stats.get('last_page_total', 0),
                                        est_pages=effective_max_pages
                                    )
                                    # æ¼æŠ“é¢„è­¦ï¼šè‹¥æœªåˆ°ä¸‹ç•Œä¸”å·²ç”¨å®Œä¼°ç®—é¡µæ•°
                                    if not stats.get('reached_lower_bound') and stats.get('used_pages') >= effective_max_pages:
                                        self.logger.warning(f"âš ï¸ è´¦å· {target['name']} å¯èƒ½æœªè§¦è¾¾æ—¶é—´ä¸‹ç•Œï¼Œå»ºè®®æå‡ä¼°ç®—æˆ–å¢é‡ç¿»é¡µ (used_pages={stats.get('used_pages')}, est={effective_max_pages})")
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ æ›´æ–°è‡ªé€‚åº”ç»Ÿè®¡å¤±è´¥: {e}")
                            break  # æˆåŠŸå®Œæˆï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                        except Exception as e:
                            self.logger.error(f"âŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                            if attempt < max_attempts - 1:
                                self.logger.info("ğŸ”„ å‡†å¤‡é‡è¯•...")
                                time.sleep(5)
                            else:
                                self.logger.error("âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†")

                    if not batch_spider or not batch_spider.articles_data:
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' çˆ¬å–å¤±è´¥")
                        failed_count += 1
                        continue

                    if batch_spider.articles_data:
                        # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ å…¬ä¼—å·ä¿¡æ¯
                        for article in batch_spider.articles_data:
                            article['å…¬ä¼—å·åç§°'] = target['name']
                            article['å…¬ä¼—å·åºå·'] = i

                        all_results.extend(batch_spider.articles_data)

                        # ä¿å­˜å½“å‰å…¬ä¼—å·çš„æ•°æ®
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        excel_file = batch_spider.save_to_excel(f"./data/readnum_batch/readnum_{target['name']}_{timestamp}.xlsx")
                        json_file = batch_spider.save_to_json(f"./data/readnum_batch/readnum_{target['name']}_{timestamp}.json")

                        self.logger.info(f"âœ… å…¬ä¼—å· '{target['name']}' çˆ¬å–å®Œæˆï¼è·å– {len(batch_spider.articles_data)} ç¯‡æ–‡ç« ")
                        self.logger.info(f"ğŸ“Š æ•°æ®å·²ä¿å­˜åˆ°: {excel_file}")
                        successful_count += 1
                    else:
                        self.logger.warning(f"âš ï¸ å…¬ä¼—å· '{target['name']}' æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®")
                        failed_count += 1

                    # å…¬ä¼—å·é—´å»¶è¿Ÿï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    if i < len(all_targets):
                        self.logger.info(f"â³ å…¬ä¼—å·é—´å»¶è¿Ÿ {self.account_delay} ç§’...")
                        time.sleep(self.account_delay)

                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†å…¬ä¼—å· '{target['name']}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # ç¡®ä¿åœæ­¢æŠ“å–å™¨
                    if cookie_reader:
                        try:
                            cookie_reader.stop_cookie_extractor()
                        except:
                            pass
                    failed_count += 1
                    continue

        except Exception as e:
            self.logger.error(f"âŒ è‡ªåŠ¨åŒ–æµç¨‹å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

        # æ±‡æ€»ç»“æœ
        self.logger.info("="*80)
        self.logger.info("ğŸ“Š å¤šå…¬ä¼—å·çˆ¬å–æ±‡æ€»ç»“æœ")
        self.logger.info("="*80)
        self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {successful_count} ä¸ªå…¬ä¼—å·")
        self.logger.info(f"âŒ å¤±è´¥å¤„ç†: {failed_count} ä¸ªå…¬ä¼—å·")
        self.logger.info(f"ğŸ“„ æ€»è®¡æ–‡ç« : {len(all_results)} ç¯‡")

        # è‹¥åˆ†æ®µé˜¶æ®µæˆåŠŸä¸”è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·æœ‰æ•°æ®ï¼Œåˆ™æ ‡è®°å®Œæˆé˜¶æ®µ
        if stage_info and successful_count > 0:
            # ç²—ç•¥ç»Ÿè®¡ï¼šä½¿ç”¨çš„é¡µæ•° = æœ€åä¸€ç¯‡æ–‡ç« æ‰€åœ¨çš„é¡µé¢ä¼°è®¡ï¼ˆæ— æ³•ç²¾ç¡®ï¼Œåç»­å¯åœ¨ spider å†…è¿”å›ï¼‰
            backfill_mgr.mark_completed(stage_info)
            self.logger.info(f"ğŸ§© å·²æ ‡è®°é˜¶æ®µå®Œæˆ: {stage_label}")

        # æ±‡æ€»æ•°æ®ä¿å­˜åŠŸèƒ½å·²ç§»é™¤

        self.logger.info("="*80)
        self.logger.info("âœ… å¤šå…¬ä¼—å·å…¨æ–°è‡ªåŠ¨åŒ–æµç¨‹æ‰§è¡Œå®Œæ¯• âœ…")
        self.logger.info("="*80)

        return successful_count > 0  # åªè¦æœ‰ä¸€ä¸ªæˆåŠŸå°±ç®—æˆåŠŸ
