# coding:utf-8
# enhanced_wx_crawler.py - å¢å¼ºç‰ˆå¾®ä¿¡æ–‡ç« é“¾æ¥æŠ“å–å™¨
import os
import requests
import json
import urllib3
import utils
import pandas as pd
from datetime import datetime
import time
import random
import re
from bs4 import BeautifulSoup
import html
from database_manager import DatabaseManager


class EnhancedWxCrawler(object):
    """å¢å¼ºç‰ˆç¿»é¡µå†…å®¹æŠ“å–ï¼Œæ”¯æŒä¿å­˜åˆ°æ–‡ä»¶"""
    urllib3.disable_warnings()

    def __init__(self, appmsg_token, biz, cookie, begin_page_index=0, end_page_index=5, save_to_file=True, get_content=True,
                 unit_name="", save_to_db=False, db_config=None):
        # èµ·å§‹é¡µæ•°
        self.begin_page_index = begin_page_index
        # ç»“æŸé¡µæ•°
        self.end_page_index = end_page_index
        # æŠ“äº†å¤šå°‘æ¡äº†
        self.num = 1
        # æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
        self.save_to_file = save_to_file
        # æ˜¯å¦è·å–æ–‡ç« å†…å®¹
        self.get_content = get_content
        # å­˜å‚¨æŠ“å–çš„æ–‡ç« æ•°æ®
        self.articles_data = []
        # å•ä½åç§°ï¼ˆå…¬ä¼—å·åç§°ï¼‰
        self.unit_name = unit_name
        # æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        self.save_to_db = save_to_db
        # æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = None

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        if self.save_to_db:
            try:
                if db_config:
                    self.db_manager = DatabaseManager(**db_config)
                else:
                    self.db_manager = DatabaseManager()  # ä½¿ç”¨é»˜è®¤é…ç½®
                print("âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹ï¼Œå°†å®æ—¶ä¿å­˜æ–‡ç« æ•°æ®")
            except Exception as e:
                print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                print("âš ï¸ å°†åªä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“")
                self.save_to_db = False

        self.appmsg_token = appmsg_token
        self.biz = biz
        self.headers = {
            "User-Agent": "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/57.0.2987.132 MQQBrowser/6.2 Mobile",
            "Cookie": cookie
        }
        self.cookie = cookie

    def article_list(self, context):
        """è§£ææ–‡ç« åˆ—è¡¨"""
        try:
            articles = json.loads(context).get('general_msg_list')
            return json.loads(articles)
        except Exception as e:
            print(f"âŒ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def extract_articles_from_page(self, articles_data):
        """ä»é¡µé¢æ•°æ®ä¸­æå–æ–‡ç« ä¿¡æ¯"""
        extracted_articles = []

        if not articles_data or 'list' not in articles_data:
            return extracted_articles

        for a in articles_data['list']:
            # æ”¶é›†æ‰€æœ‰æ–‡ç« ï¼ˆä¸»æ¡å’Œå‰¯æ¡ç»Ÿä¸€å¤„ç†ï¼‰
            all_articles = []

            # æ·»åŠ ä¸»æ¡æ–‡ç« 
            if 'app_msg_ext_info' in a.keys() and '' != a.get('app_msg_ext_info').get('content_url',''):
                main_article = {
                    'title': a.get('app_msg_ext_info').get('title', ''),
                    'url': a.get('app_msg_ext_info').get('content_url', ''),
                    'pub_time': self.format_time(a.get('comm_msg_info', {}).get('datetime', 0)),
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                all_articles.append(main_article)

            # æ·»åŠ å‰¯æ¡æ–‡ç« 
            if 'app_msg_ext_info' in a.keys():
                for m in a.get('app_msg_ext_info').get('multi_app_msg_item_list',[]):
                    sub_article = {
                        'title': m.get('title', ''),
                        'url': m.get('content_url', ''),
                        'pub_time': self.format_time(a.get('comm_msg_info', {}).get('datetime', 0)),
                        'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    all_articles.append(sub_article)

            # ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ–‡ç« 
            for article_info in all_articles:
                # æ¯ç¯‡æ–‡ç« å¤„ç†å‰æ·»åŠ å»¶è¿Ÿ (10-20ç§’)
                delay = random.randint(10, 20)
                print(f"æ¯ç¯‡æ–‡ç« é—´ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)

                # å¦‚æœéœ€è¦è·å–æ–‡ç« å†…å®¹
                if self.get_content:
                    content_data = self.get_article_content(article_info['url'])
                    if content_data:
                        if content_data.get('error') == 'captcha_required':
                            print("ğŸ›‘ é‡åˆ°éªŒè¯ç ï¼Œåœæ­¢è·å–å†…å®¹")
                            return extracted_articles
                        elif content_data.get('error') == 'invalid_params':
                            print("âš ï¸ URLå‚æ•°é”™è¯¯ï¼Œè·³è¿‡æ­¤æ–‡ç« å†…å®¹è·å–")
                        elif content_data.get('error') == 'not_article_page':
                            print("âš ï¸ éæ–‡ç« é¡µé¢ï¼Œè·³è¿‡å†…å®¹è·å–")
                        else:
                            article_info.update({
                                'content': content_data.get('content', ''),
                                'content_length': content_data.get('content_length', 0),
                            })

                # æ·»åŠ å•ä½åç§°
                article_info['unit_name'] = self.unit_name

                # å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“
                if self.save_to_db and self.db_manager:
                    try:
                        success = self.db_manager.insert_article(article_info)
                        if success:
                            print(f"ğŸ’¾ ç¬¬{self.num}æ¡æ–‡ç« å·²ä¿å­˜åˆ°æ•°æ®åº“: {article_info['title']}")
                        else:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ ‡é¢˜é‡å¤è€Œè·³è¿‡
                            if article_info.get('title', '').strip() and self.db_manager.check_article_title_exists(article_info.get('title', '').strip()):
                                print(f"âš ï¸ ç¬¬{self.num}æ¡æ–‡ç« æ ‡é¢˜é‡å¤ï¼Œå·²è·³è¿‡: {article_info['title']}")
                            else:
                                print(f"âŒ ç¬¬{self.num}æ¡æ–‡ç« æ•°æ®åº“ä¿å­˜å¤±è´¥: {article_info['title']}")
                    except Exception as e:
                        print(f"âŒ æ•°æ®åº“ä¿å­˜å‡ºé”™: {e}")

                extracted_articles.append(article_info)
                print(f"{self.num}æ¡ {article_info['title']}")
                self.num += 1

        return extracted_articles

    def validate_and_fix_url(self, url):
        """
        éªŒè¯å’Œä¿®å¤æ–‡ç« URL
        :param url: åŸå§‹URL
        :return: ä¿®å¤åçš„URLæˆ–None
        """
        if not url:
            return None

        # HTMLè§£ç 
        url = html.unescape(url)

        # ç§»é™¤å¯èƒ½çš„HTMLè½¬ä¹‰å­—ç¬¦
        url = url.replace('&amp;', '&')

        # æ£€æŸ¥URLæ˜¯å¦å®Œæ•´
        if not url.startswith('http'):
            print(f"âš ï¸ URLæ ¼å¼ä¸æ­£ç¡®: {url[:50]}...")
            return None

        # æ£€æŸ¥URLé•¿åº¦ï¼Œå¾®ä¿¡æ–‡ç« URLé€šå¸¸æ¯”è¾ƒé•¿
        if len(url) < 50:
            print(f"âš ï¸ URLè¿‡çŸ­ï¼Œå¯èƒ½ä¸å®Œæ•´: {url}")
            return None

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å‚æ•°
        required_params = ['__biz', 'mid', 'idx', 'sn']
        missing_params = []

        for param in required_params:
            if param not in url:
                missing_params.append(param)

        if missing_params:
            print(f"âš ï¸ URLç¼ºå°‘å¿…è¦å‚æ•°: {missing_params}")
            print(f"   URL: {url[:100]}...")
            return None

        # ç¡®ä¿URLä»¥httpså¼€å¤´
        if url.startswith('http://'):
            url = url.replace('http://', 'https://', 1)

        return url

    def format_time(self, timestamp):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        try:
            if timestamp:
                return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
            else:
                return ''
        except:
            return ''

    def get_article_content(self, article_url):
        """
        è·å–æ–‡ç« å†…å®¹
        :param article_url: æ–‡ç« URL
        :return: æ–‡ç« å†…å®¹å­—å…¸
        """
        if not article_url:
            return None

        # éªŒè¯å’Œä¿®å¤URL
        article_url = self.validate_and_fix_url(article_url)
        if not article_url:
            print("âŒ URLæ— æ•ˆï¼Œè·³è¿‡")
            return None

        try:
            print(f"ğŸ“„ è·å–æ–‡ç« å†…å®¹: {article_url[:50]}...")
            print(f"ğŸ”— å®Œæ•´URL: {article_url}")

            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å° (5-10ç§’ï¼Œå› ä¸ºæ–‡ç« å¤„ç†å‰å·²æœ‰å»¶è¿Ÿ)
            delay = random.randint(1,5)
            print(f"è·å–å†…å®¹å‰ç­‰å¾… {delay} ç§’...")
            time.sleep(delay)

            response = requests.get(
                article_url,
                headers=self.headers,
                verify=False,
                timeout=30
            )

            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None

            html_content = response.text

            # æ£€æŸ¥å„ç§é”™è¯¯æƒ…å†µ
            if "å‚æ•°é”™è¯¯" in html_content:
                print("âŒ å¾®ä¿¡è¿”å›'å‚æ•°é”™è¯¯'ï¼ŒURLå¯èƒ½ä¸å®Œæ•´æˆ–å·²å¤±æ•ˆ")
                print(f"   é—®é¢˜URL: {article_url}")
                return {
                    'content': '',
                    'error': 'invalid_params'
                }

            # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç é¡µé¢
            if "ç¯å¢ƒå¼‚å¸¸" in html_content or "å®ŒæˆéªŒè¯" in html_content or "secitptpage/verify" in html_content:
                print("âš ï¸ é‡åˆ°å¾®ä¿¡éªŒè¯ç é¡µé¢ï¼Œéœ€è¦æ‰‹åŠ¨éªŒè¯")
                return {
                    'content': '',
                    'error': 'captcha_required'
                }

            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸå®æ–‡ç« é¡µé¢
            if "js_content" not in html_content and "rich_media_content" not in html_content:
                print("âš ï¸ éæ–‡ç« é¡µé¢ï¼Œå¯èƒ½è¢«é‡å®šå‘æˆ–æ–‡ç« ä¸å­˜åœ¨")
                return {
                    'content': '',
                    'error': 'not_article_page'
                }

            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')

            # æå–æ–‡ç« æ ‡é¢˜
            title = ""
            title_tag = soup.find('h1', {'class': 'rich_media_title'}) or soup.find('meta', {'property': 'og:title'})
            if title_tag:
                if title_tag.name == 'meta':
                    title = title_tag.get('content', '')
                else:
                    title = title_tag.get_text(strip=True)

            # æå–æ–‡ç« ä½œè€…
            author = ""
            author_tag = soup.find('a', {'class': 'rich_media_meta_link'}) or soup.find('meta', {'property': 'og:article:author'})
            if author_tag:
                if author_tag.name == 'meta':
                    author = author_tag.get('content', '')
                else:
                    author = author_tag.get_text(strip=True)

            # æå–æ–‡ç« å†…å®¹
            content = ""
            content_div = soup.find('div', {'class': 'rich_media_content'}) or soup.find('div', {'id': 'js_content'})
            if content_div:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_div(["script", "style"]):
                    script.decompose()

                # è·å–çº¯æ–‡æœ¬å†…å®¹
                content = content_div.get_text(separator='\n', strip=True)

                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
                content = re.sub(r'\n\s*\n', '\n\n', content)
                content = content.strip()

            # æå–å‘å¸ƒæ—¶é—´
            pub_time = ""
            time_tag = soup.find('em', {'class': 'rich_media_meta_text'}) or soup.find('span', {'class': 'rich_media_meta_text'})
            if time_tag:
                pub_time = time_tag.get_text(strip=True)

            result = {
                'title': title,
                'author': author,
                'content': content,
                'pub_time': pub_time,
                'content_length': len(content),
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            print(f"âœ… å†…å®¹è·å–æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return result

        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return None

    def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not self.articles_data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return None, None

        # æ ¹æ®æ˜¯å¦è·å–å†…å®¹é€‰æ‹©ä¸åŒçš„ä¿å­˜ç›®å½•
        if self.get_content:
            data_dir = "./data/with_content"
            file_prefix = "articles_with_content"
        else:
            data_dir = "./data/basic_links"
            file_prefix = "article_links"

        os.makedirs(data_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_file = os.path.join(data_dir, f"{file_prefix}_{timestamp}.xlsx")
        json_file = os.path.join(data_dir, f"{file_prefix}_{timestamp}.json")

        try:
            # åˆ›å»ºè¿‡æ»¤åçš„æ•°æ®å‰¯æœ¬ï¼Œå»é™¤authorå’Œdetailed_pub_timeåˆ—
            filtered_data = []
            for article in self.articles_data:
                filtered_article = {}
                for key, value in article.items():
                    # æ’é™¤authorå’Œdetailed_pub_timeå­—æ®µ
                    if key not in ['author', 'detailed_pub_time']:
                        filtered_article[key] = value
                filtered_data.append(filtered_article)

            # ä¿å­˜ä¸ºExcel
            df = pd.DataFrame(filtered_data)
            df.to_excel(excel_file, index=False, engine='openpyxl')
            print(f"ğŸ“Š Excelæ–‡ä»¶å·²ä¿å­˜: {excel_file}")

            # ä¿å­˜ä¸ºJSON
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")

            return excel_file, json_file

        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None, None

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        current_page = self.begin_page_index

        print(f"ğŸš€ å¼€å§‹æŠ“å–æ–‡ç« {'å†…å®¹' if self.get_content else 'é“¾æ¥'}...")
        print(f"ğŸ“‹ é¡µæ•°èŒƒå›´: {self.begin_page_index} - {self.end_page_index}")
        if self.get_content:
            print(f"ğŸ“„ å°†è·å–æ–‡ç« å®Œæ•´å†…å®¹ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        else:
            print(f"ğŸ”— ä»…è·å–æ–‡ç« é“¾æ¥ä¿¡æ¯ï¼ˆé€Ÿåº¦è¾ƒå¿«ï¼‰")

        while current_page <= self.end_page_index:
            try:
                print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {current_page + 1} é¡µ...")
                
                # ç¿»é¡µåœ°å€
                page_url = "https://mp.weixin.qq.com/mp/profile_ext?action=getmsg&__biz={}&f=json&offset={}&count=10&is_ok=1&scene=&uin=777&key=777&pass_ticket={}&wxtoken=&appmsg_token={}&x5=0&f=json"
                
                # å°† cookie å­—ç¬¦ä¸²æ¸…ç†å¹¶å­—å…¸åŒ–
                clean_cookie = self.cookie.replace('\u00a0', ' ').strip()
                wx_dict = utils.str_to_dict(clean_cookie, join_symbol='; ', split_symbol='=')
                
                # è¯·æ±‚åœ°å€
                response = requests.get(
                    page_url.format(
                        self.biz, 
                        current_page * 10, 
                        wx_dict['pass_ticket'], 
                        self.appmsg_token
                    ), 
                    headers=self.headers, 
                    verify=False,
                    timeout=30
                )
                
                if response.status_code != 200:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break
                
                # å°†æ–‡ç« åˆ—è¡¨å­—å…¸åŒ–
                articles = self.article_list(response.text)
                
                if not articles:
                    print("âŒ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥ï¼Œå¯èƒ½Cookieå·²è¿‡æœŸ")
                    break
                
                # æå–æ–‡ç« ä¿¡æ¯
                page_articles = self.extract_articles_from_page(articles)
                
                if not page_articles:
                    print("âš ï¸ æœ¬é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                    break
                
                # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                self.articles_data.extend(page_articles)
                print(f"âœ… ç¬¬ {current_page + 1} é¡µå®Œæˆï¼Œè·å– {len(page_articles)} ç¯‡æ–‡ç« ")
                
                current_page += 1
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                if current_page <= self.end_page_index:
                    print("â³ ç­‰å¾… 3 ç§’...")
                    time.sleep(3)
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æŠ“å–")
                break
            except Exception as e:
                print(f"âŒ æŠ“å–ç¬¬ {current_page + 1} é¡µæ—¶å‡ºé”™: {e}")
                current_page += 1
                continue
        
        # ä¿å­˜æ•°æ®
        if self.save_to_file and self.articles_data:
            self.save_data()

        # å…³é—­æ•°æ®åº“è¿æ¥
        if self.db_manager:
            self.db_manager.disconnect()
            print("ğŸ’¾ æ•°æ®åº“è¿æ¥å·²å…³é—­")

        print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±è·å– {len(self.articles_data)} ç¯‡æ–‡ç« é“¾æ¥")
        if self.save_to_db:
            print(f"ğŸ’¾ æ•°æ®å·²å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“")

        return self.articles_data

    def print_summary(self):
        """æ‰“å°æŠ“å–æ‘˜è¦"""
        if not self.articles_data:
            print("ğŸ“Š æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")
            return

        print(f"\nğŸ“Š æŠ“å–æ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“– æ€»æ–‡ç« æ•°: {len(self.articles_data)}")

        # å¦‚æœè·å–äº†å†…å®¹ï¼Œæ˜¾ç¤ºå†…å®¹ç»Ÿè®¡
        if self.get_content:
            content_articles = [a for a in self.articles_data if a.get('content')]
            print(f"ğŸ“„ æˆåŠŸè·å–å†…å®¹: {len(content_articles)} ç¯‡")
            if content_articles:
                avg_length = sum(a.get('content_length', 0) for a in content_articles) / len(content_articles)
                print(f"ï¿½ å¹³å‡å†…å®¹é•¿åº¦: {int(avg_length)} å­—ç¬¦")

        # æ˜¾ç¤ºæœ€æ–°å‡ ç¯‡æ–‡ç« 
        print(f"\nğŸ“‹ æœ€æ–°æ–‡ç« :")
        for i, article in enumerate(self.articles_data[:5]):
            title = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
            if self.get_content and article.get('content_length'):
                print(f"   {i+1}. {title} ({article['content_length']}å­—)")
            else:
                print(f"   {i+1}. {title}")

        if len(self.articles_data) > 5:
            print(f"   ... è¿˜æœ‰ {len(self.articles_data) - 5} ç¯‡æ–‡ç« ")
