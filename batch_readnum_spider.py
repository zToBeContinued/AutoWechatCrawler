#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ‰¹é‡é˜…è¯»é‡æŠ“å–å™¨
åŸºäºç°æœ‰ä»£ç æ¶æ„ï¼Œæ•´åˆæ–‡ç« é“¾æ¥è·å–å’Œé˜…è¯»é‡æŠ“å–åŠŸèƒ½
"""

import os
import re
import json
import time
import random
import requests
import pandas as pd
import winreg
import ctypes
import contextlib
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from read_cookie import ReadCookie
import utils
from database_manager import DatabaseManager

class BatchReadnumSpider:
    """æ‰¹é‡å¾®ä¿¡å…¬ä¼—å·é˜…è¯»é‡æŠ“å–å™¨"""
    
    def __init__(self, auth_info: dict = None, save_to_db=False, db_config=None, unit_name=""):
        """
        åˆå§‹åŒ–æ‰¹é‡é˜…è¯»é‡æŠ“å–å™¨
        :param auth_info: åŒ…å«appmsg_token, biz, cookie_strå’Œheadersçš„å­—å…¸
        :param save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        :param db_config: æ•°æ®åº“é…ç½®
        :param unit_name: å•ä½åç§°ï¼ˆå…¬ä¼—å·åç§°ï¼‰
        """
        # åˆå§‹åŒ–è®¤è¯ä¿¡æ¯
        self.appmsg_token = None
        self.biz = None
        self.cookie_str = None
        self.auth_info = auth_info # å­˜å‚¨ä¼ å…¥çš„è®¤è¯æ•°æ®

        # æ•°æ®åº“ç›¸å…³é…ç½®
        self.save_to_db = save_to_db
        self.unit_name = unit_name
        self.db_manager = None

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        if self.save_to_db:
            try:
                if db_config:
                    self.db_manager = DatabaseManager(**db_config)
                else:
                    self.db_manager = DatabaseManager()  # ä½¿ç”¨é»˜è®¤é…ç½®
                print("âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹ï¼Œå°†å®æ—¶ä¿å­˜æ–‡ç« æ•°æ®")
            except Exception as e:
                print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                print("âš ï¸ å°†åªä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“")
                self.save_to_db = False

        # è¯·æ±‚å¤´é…ç½® - å‚è€ƒspider_readnum.pyçš„æˆåŠŸå®ç°
        self.headers = {
            'cache-control': 'max-age=0',
            'x-wechat-key': '',
            'x-wechat-uin': '',
            'exportkey': '',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090c37) XWEB/14315 Flue',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/wxpic,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'sec-fetch-site': 'same-origin',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-dest': 'document',
            'accept-encoding': 'gzip, deflate, br',
            'accept-language': 'zh-CN,zh;q=0.9',
            'priority': 'u=0, i',
        }

        # ä»ä¼ å…¥çš„auth_infoåŠ è½½è®¤è¯ä¿¡æ¯å’Œæ›´æ–°headers
        if self.auth_info:
            self.load_auth_info()
        else:
            print("âŒ BatchReadnumSpider åˆå§‹åŒ–æ—¶æœªæä¾›è®¤è¯æ•°æ®ã€‚")

        # æ•°æ®å­˜å‚¨ - ç»Ÿä¸€å­˜å‚¨æ‰€æœ‰å­—æ®µ
        self.articles_data = []

        # é¢‘ç‡æ§åˆ¶
        self.request_count = 0
        self.last_request_time = 0
        self.min_interval = 3  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

        # åˆ›å»ºæ•°æ®ç›®å½•
        os.makedirs("./data/readnum_batch", exist_ok=True)
        
    def load_auth_info(self):
        """ä»ä¼ å…¥çš„è®¤è¯æ•°æ®åŠ è½½è®¤è¯ä¿¡æ¯å’Œheaders"""
        if not self.auth_info:
            print("âŒ æœªä¼ å…¥æœ‰æ•ˆçš„è®¤è¯æ•°æ®ï¼Œæ— æ³•åŠ è½½è®¤è¯ä¿¡æ¯ã€‚")
            return False

        try:
            self.appmsg_token = self.auth_info.get('appmsg_token')
            self.biz = self.auth_info.get('biz')
            self.cookie_str = self.auth_info.get('cookie_str')

            # æ›´æ–°è¯·æ±‚å¤´ï¼Œä½¿ç”¨æŠ“åŒ…è·å–çš„çœŸå®headers
            captured_headers = self.auth_info.get('headers', {})
            if captured_headers:
                # ä½¿ç”¨æŠ“åŒ…è·å–çš„æ‰€æœ‰headersè¦†ç›–é»˜è®¤å€¼
                # ç‰¹åˆ«é‡è¦çš„æ˜¯x-wechat-keyï¼Œè¿™æ˜¯è·å–é˜…è¯»é‡çš„å…³é”®
                for key, value in captured_headers.items():
                    self.headers[key] = value

                # æ£€æŸ¥å…³é”®çš„headersæ˜¯å¦å­˜åœ¨
                key_headers = ['x-wechat-key', 'x-wechat-uin', 'exportkey']
                missing_headers = [h for h in key_headers if h not in captured_headers]

                if missing_headers:
                    print(f"âš ï¸ ç¼ºå°‘å…³é”®headers: {missing_headers}")
                    # å¦‚æœç¼ºå°‘x-wechat-keyï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆæ¥è‡ªspider_readnum.pyçš„æˆåŠŸå®ç°ï¼‰
                    if 'x-wechat-key' in missing_headers:
                        print("ğŸ”‘ ä½¿ç”¨é»˜è®¤çš„x-wechat-keyå€¼")
                else:
                    print(f"âœ… å·²æ›´æ–°æ‰€æœ‰ {len(captured_headers)} ä¸ªè¯·æ±‚å¤´å‚æ•°ï¼ŒåŒ…å«å…³é”®çš„x-wechat-key")

                # æ˜¾ç¤ºx-wechat-keyçš„å‰20ä¸ªå­—ç¬¦ç”¨äºéªŒè¯
                if 'x-wechat-key' in captured_headers:
                    print(f"ğŸ”‘ x-wechat-key: {captured_headers['x-wechat-key'][:20]}...")
                elif 'x-wechat-key' in self.headers:
                    print(f"ğŸ”‘ ä½¿ç”¨é»˜è®¤x-wechat-key: {self.headers['x-wechat-key'][:20]}...")
            else:
                print("âš ï¸ æœªè·å–åˆ°headersä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤çš„x-wechat-key")
                print(f"ğŸ”‘ é»˜è®¤x-wechat-key: {self.headers['x-wechat-key'][:20]}...")

            print(f"âœ… æˆåŠŸåŠ è½½è®¤è¯ä¿¡æ¯")
            print(f"   __biz: {self.biz}")
            print(f"   appmsg_token: {self.appmsg_token[:20]}...")
            print(f"   headers: {list(captured_headers.keys())}")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½è®¤è¯ä¿¡æ¯å¤±è´¥: {e}")
            return False

    def validate_cookie(self):
        """
        éªŒè¯Cookieæ˜¯å¦æœ‰æ•ˆ
        :return: æ˜¯å¦æœ‰æ•ˆ
        """
        print("ğŸ” éªŒè¯Cookieæœ‰æ•ˆæ€§...")

        if not all([self.appmsg_token, self.biz, self.cookie_str]):
            print("âŒ è®¤è¯ä¿¡æ¯ä¸å®Œæ•´")
            return False

        try:
            # å°è¯•è·å–ç¬¬ä¸€é¡µæ–‡ç« åˆ—è¡¨æ¥éªŒè¯Cookie
            print("ğŸ” å°è¯•è·å–æ–‡ç« åˆ—è¡¨ä»¥éªŒè¯Cookie...")
            test_articles = self.get_article_list(begin_page=0, count=1)
            if test_articles:
                print("âœ… CookieéªŒè¯æˆåŠŸ")
                return True
            else:
                print("âŒ CookieéªŒè¯å¤±è´¥ï¼Œå¯èƒ½å·²è¿‡æœŸ")
                return False
        except Exception as e:
            print(f"âŒ CookieéªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False


    
    @contextlib.contextmanager
    def manage_system_proxy(self, proxy_address="127.0.0.1:8080"):
        """
        ä»£ç†ç®¡ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¸´æ—¶ç¦ç”¨ç³»ç»Ÿä»£ç†
        """
        INTERNET_OPTION_SETTINGS_CHANGED = 39
        INTERNET_OPTION_REFRESH = 37
        InternetSetOption = ctypes.windll.wininet.InternetSetOptionW
        
        original_state = {"enabled": False, "server": ""}
        was_active = False
        key = None
        
        try:
            # æ‰“å¼€æ³¨å†Œè¡¨é¡¹
            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 
                               r"Software\Microsoft\Windows\CurrentVersion\Internet Settings", 
                               0, winreg.KEY_READ | winreg.KEY_WRITE)
            
            # è¯»å–åŸå§‹ä»£ç†çŠ¶æ€
            try:
                original_state["enabled"] = winreg.QueryValueEx(key, "ProxyEnable")[0] == 1
                original_state["server"] = winreg.QueryValueEx(key, "ProxyServer")[0]
            except FileNotFoundError:
                pass
            
            # æ£€æŸ¥ä»£ç†æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦ç¦ç”¨çš„é‚£ä¸ª
            if original_state["enabled"] and original_state["server"] == proxy_address:
                was_active = True
                print(f"ğŸ”§ æ£€æµ‹åˆ°æ´»åŠ¨ä»£ç† {proxy_address}ï¼Œæ­£åœ¨ä¸´æ—¶ç¦ç”¨...")
                winreg.SetValueEx(key, "ProxyEnable", 0, winreg.REG_DWORD, 0)
                InternetSetOption(0, INTERNET_OPTION_SETTINGS_CHANGED, 0, 0)
                InternetSetOption(0, INTERNET_OPTION_REFRESH, 0, 0)
            
            yield  # æ‰§è¡Œä¸»ä»£ç å—
            
        finally:
            # æ¢å¤åŸå§‹ä»£ç†è®¾ç½®
            if was_active and key:
                print(f"ğŸ”§ æ­£åœ¨æ¢å¤ä»£ç† {proxy_address}...")
                winreg.SetValueEx(key, "ProxyEnable", 0, winreg.REG_DWORD, 1)
                InternetSetOption(0, INTERNET_OPTION_SETTINGS_CHANGED, 0, 0)
                InternetSetOption(0, INTERNET_OPTION_REFRESH, 0, 0)
            if key:
                winreg.CloseKey(key)
    
    def rate_limit(self):
        """æ™ºèƒ½é¢‘ç‡æ§åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_interval:
            sleep_time = self.min_interval - time_since_last
            print(f"â³ é¢‘ç‡æ§åˆ¶ï¼šç­‰å¾… {sleep_time:.1f} ç§’...")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
        
        # æ¯10ä¸ªè¯·æ±‚å¢åŠ é¢å¤–å»¶è¿Ÿ
        if self.request_count % 10 == 0:
            extra_delay = random.randint(5, 10)
            print(f"â³ ç¬¬{self.request_count}ä¸ªè¯·æ±‚ï¼Œé¢å¤–å»¶è¿Ÿ {extra_delay} ç§’...")
            time.sleep(extra_delay)
    
    def get_article_list(self, begin_page=0, count=10):
        """
        è·å–æ–‡ç« åˆ—è¡¨
        :param begin_page: èµ·å§‹é¡µæ•°
        :param count: æ¯é¡µæ–‡ç« æ•°é‡
        :return: æ–‡ç« åˆ—è¡¨
        """
        if not all([self.appmsg_token, self.biz, self.cookie_str]):
            print("âŒ è®¤è¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œæ— æ³•è·å–æ–‡ç« åˆ—è¡¨")
            return []
        
        # é¢‘ç‡æ§åˆ¶
        self.rate_limit()
        
        # æ„å»ºè¯·æ±‚URL
        page_url = "https://mp.weixin.qq.com/mp/profile_ext"
        params = {
            "action": "getmsg",
            "__biz": self.biz,
            "f": "json",
            "offset": begin_page * count,
            "count": count,
            "is_ok": 1,
            "scene": "",
            "uin": "777",
            "key": "777",
            "pass_ticket": "",
            "wxtoken": "",
            "appmsg_token": self.appmsg_token,
            "x5": 0
        }
        
        # è§£æcookie
        clean_cookie = self.cookie_str.replace('\u00a0', ' ').strip()
        cookie_dict = utils.str_to_dict(clean_cookie, join_symbol='; ', split_symbol='=')
        
        if 'pass_ticket' in cookie_dict:
            params['pass_ticket'] = cookie_dict['pass_ticket']
        
        # æ›´æ–°è¯·æ±‚å¤´
        headers = self.headers.copy()
        headers["Cookie"] = self.cookie_str
        
        try:
            print(f"ğŸ“¡ è·å–æ–‡ç« åˆ—è¡¨ï¼šç¬¬{begin_page+1}é¡µï¼Œæ¯é¡µ{count}ç¯‡")
            
            response = requests.get(page_url, params=params, headers=headers, verify=False, timeout=30)
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
            
            # è§£æå“åº”
            try:
                content_json = response.json()
            except:
                print("âŒ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                print(f"ğŸ” å“åº”å†…å®¹å‰500å­—ç¬¦: {response.text[:500]}")
                return []

            # è°ƒè¯•ï¼šæ‰“å°å“åº”çš„å…³é”®ä¿¡æ¯
            print(f"ğŸ” å“åº”çŠ¶æ€: {response.status_code}")
            print(f"ğŸ” å“åº”é”®: {list(content_json.keys())}")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "base_resp" in content_json:
                base_resp = content_json["base_resp"]
                print(f"ğŸ” base_resp: {base_resp}")
                if base_resp.get("err_msg") == "freq control":
                    print("âš ï¸ é‡åˆ°é¢‘ç‡æ§åˆ¶é™åˆ¶ï¼Œå»ºè®®ç¨åé‡è¯•")
                    return []
                elif base_resp.get("ret") != 0:
                    print(f"âŒ APIè¿”å›é”™è¯¯: ret={base_resp.get('ret')}, err_msg={base_resp.get('err_msg')}")
                    return []

            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
            if content_json.get("ret") == -3:
                print("âŒ CookieéªŒè¯å¤±è´¥ï¼Œå¯èƒ½å·²è¿‡æœŸ")
                print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                print("   1. Cookieå·²è¿‡æœŸï¼ˆé€šå¸¸24å°æ—¶åè¿‡æœŸï¼‰")
                print("   2. Cookieæ ¼å¼ä¸æ­£ç¡®æˆ–è¢«æˆªæ–­")
                print("   3. å¾®ä¿¡æ£€æµ‹åˆ°å¼‚å¸¸è®¿é—®æ¨¡å¼")
                print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print("   1. é‡æ–°è¿è¡Œç¨‹åºè·å–æ–°çš„Cookie")
                print("   2. ç¡®ä¿åœ¨å¾®ä¿¡ä¸­æ­£å¸¸è®¿é—®æ–‡ç« åå†æŠ“å–")
                print("   3. é™ä½æŠ“å–é¢‘ç‡ï¼Œå¢åŠ å»¶è¿Ÿæ—¶é—´")
                return []

            # è§£ææ–‡ç« åˆ—è¡¨
            if "general_msg_list" not in content_json:
                print("âŒ å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
                print(f"ğŸ” å®Œæ•´å“åº”: {content_json}")
                return []
            
            articles_json = json.loads(content_json["general_msg_list"])
            articles = []
            
            for item in articles_json.get("list", []):
                # å¤„ç†ä¸»æ–‡ç« 
                if "app_msg_ext_info" in item and item["app_msg_ext_info"].get("content_url"):
                    main_article = item["app_msg_ext_info"]
                    articles.append({
                        "title": main_article.get("title", ""),
                        "url": main_article.get("content_url", ""),
                        "author": main_article.get("author", ""),
                        "digest": main_article.get("digest", ""),
                        "create_time": item.get("comm_msg_info", {}).get("datetime", 0)
                    })
                
                # å¤„ç†å‰¯æ–‡ç« 
                if "app_msg_ext_info" in item:
                    for sub_article in item["app_msg_ext_info"].get("multi_app_msg_item_list", []):
                        articles.append({
                            "title": sub_article.get("title", ""),
                            "url": sub_article.get("content_url", ""),
                            "author": sub_article.get("author", ""),
                            "digest": sub_article.get("digest", ""),
                            "create_time": item.get("comm_msg_info", {}).get("datetime", 0)
                        })
            
            print(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def extract_article_content_and_stats(self, article_url):
        """
        ä»æ–‡ç« é¡µé¢æå–æ–‡ç« å†…å®¹ã€é˜…è¯»é‡ã€ç‚¹èµæ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
        å‚è€ƒspider_readnum.pyçš„æˆåŠŸå®ç°
        :param article_url: æ–‡ç« URL
        :return: åŒ…å«å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not article_url:
            return None

        # é¢‘ç‡æ§åˆ¶
        self.rate_limit()

        try:
            print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡æ•°æ®: {article_url[:50]}...")

            # ä¿®å¤HTMLç¼–ç çš„URL
            import html
            clean_url = html.unescape(article_url)
            print(f"ğŸ” æ¸…ç†åURL: {clean_url}")

            # è§£æURLè·å–å‚æ•°
            from urllib.parse import urlparse, parse_qs
            parsed_url = urlparse(clean_url)
            query_params = parse_qs(parsed_url.query)

            print(f"ğŸ” è§£æåˆ°çš„å‚æ•°: {query_params}")

            # æ„å»ºè¯·æ±‚å‚æ•°ï¼Œå‚è€ƒspider_readnum.pyçš„æˆåŠŸå®ç°
            # æ³¨æ„ï¼šspider_readnum.pyä¸­å‚æ•°éƒ½æ˜¯åˆ—è¡¨æ ¼å¼
            params = {}
            for key in ['__biz', 'mid', 'idx', 'sn', 'chksm']:
                if key in query_params:
                    params[key] = query_params[key]  # parse_qså·²ç»è¿”å›åˆ—è¡¨æ ¼å¼

            # æ·»åŠ å¿…è¦çš„å‚æ•°
            # ä»cookieä¸­æå–pass_ticket
            pass_ticket_match = re.search(r'pass_ticket=([^;]+)', self.cookie_str)
            if pass_ticket_match:
                params['pass_ticket'] = [pass_ticket_match.group(1)]

            params['wx_header'] = ['1']

            print(f"ğŸ” è¯·æ±‚å‚æ•°: {params}")

            # ä½¿ç”¨å®ä¾‹çš„headersï¼ˆå·²ç»åŒ…å«äº†æŠ“åŒ…è·å–çš„å…³é”®å‚æ•°ï¼‰
            headers = self.headers.copy()

            print(f"ğŸ” ä½¿ç”¨headers: {list(headers.keys())}")

            # éªŒè¯å…³é”®çš„x-wechat-keyæ˜¯å¦å­˜åœ¨
            if 'x-wechat-key' in headers:
                print(f"ğŸ”‘ ç¡®è®¤x-wechat-keyå­˜åœ¨: {headers['x-wechat-key'][:20]}...")
            else:
                print("âŒ è­¦å‘Šï¼šx-wechat-keyä¸å­˜åœ¨ï¼Œå¯èƒ½æ— æ³•è·å–é˜…è¯»é‡æ•°æ®")

            # æ·»åŠ Cookie
            headers['Cookie'] = self.cookie_str

            # ä½¿ç”¨ä»£ç†ç®¡ç†å™¨ä¸´æ—¶ç¦ç”¨ç³»ç»Ÿä»£ç†
            with self.manage_system_proxy("127.0.0.1:8080"):
                # ä½¿ç”¨GETè¯·æ±‚è®¿é—®æ–‡ç« é¡µé¢
                base_url = "https://mp.weixin.qq.com/s"
                response = requests.get(base_url, params=params, headers=headers, timeout=30)

                if response.status_code != 200:
                    print(f"âŒ æ–‡ç« è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return None

                html_content = response.text
                print(html_content)
# ----- ä¿å­˜åˆ°html
                # ä¿å­˜HTMLå†…å®¹åˆ°debugç›®å½•
                try:
                    debug_dir = "./data/debug"
                    os.makedirs(debug_dir, exist_ok=True)
                    
                    # ç”Ÿæˆæ–‡ä»¶åï¼Œä½¿ç”¨æ—¶é—´æˆ³å’Œæ–‡ç« æ ‡é¢˜çš„å‰20ä¸ªå­—ç¬¦
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # ä»URLä¸­æå–æ–‡ç« æ ‡è¯†ç¬¦
                    import urllib.parse
                    parsed = urllib.parse.urlparse(clean_url)
                    query_params = urllib.parse.parse_qs(parsed.query)
                    mid = query_params.get('mid', ['unknown'])[0]
                    
                    filename = f"article_{timestamp}_{mid}.html"
                    filepath = os.path.join(debug_dir, filename)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    
                    print(f"ğŸ” HTMLå†…å®¹å·²ä¿å­˜åˆ°: {filepath}")
                    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦")
                    
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {e}")

                # è®°å½•HTMLé•¿åº¦ç”¨äºè°ƒè¯•
                print(f"ğŸ“ HTMLé•¿åº¦: {len(html_content)} å­—ç¬¦")

                # ------

                # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç é¡µé¢
                if "ç¯å¢ƒå¼‚å¸¸" in html_content or "å®ŒæˆéªŒè¯" in html_content or "secitptpage/verify" in html_content:
                    print("âš ï¸ é‡åˆ°å¾®ä¿¡éªŒè¯ç é¡µé¢ï¼Œéœ€è¦æ‰‹åŠ¨éªŒè¯")
                    print(f"ğŸ“„ è¯·æ‰‹åŠ¨åœ¨æµè§ˆå™¨ä¸­è®¿é—®: {article_url}")
                    print("ğŸ’¡ å»ºè®®ï¼šé™ä½æŠ“å–é¢‘ç‡ï¼Œå¢åŠ å»¶è¿Ÿæ—¶é—´")
                    return {
                        'read_count': -1,  # ç”¨-1è¡¨ç¤ºéªŒè¯ç é¡µé¢
                        'like_count': -1,
                        'share_count': -1,
                        'error': 'captcha_required'
                    }

                # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸå®æ–‡ç« é¡µé¢
                if "js_content" not in html_content and "rich_media_content" not in html_content:
                    print("âš ï¸ éæ–‡ç« é¡µé¢ï¼Œå¯èƒ½è¢«é‡å®šå‘æˆ–æ–‡ç« ä¸å­˜åœ¨")
                    return {
                        'read_count': -2,  # ç”¨-2è¡¨ç¤ºéæ–‡ç« é¡µé¢
                        'like_count': -2,
                        'share_count': -2,
                        'error': 'not_article_page'
                    }

                # æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯
                title_match = re.search(r'<meta property="og:title" content="(.*?)"', html_content)

                title = title_match.group(1) if title_match else "æœªæ‰¾åˆ°æ ‡é¢˜"

                # æå–æ–‡ç« å†…å®¹
                content = self.extract_article_content(html_content)

                # æå–å‘å¸ƒæ—¶é—´
                publish_time = self.extract_publish_time(html_content)

                # æå–å…¬ä¼—å·åç§°
                account_name = self.extract_account_name(html_content)

                # æ„å»ºå®Œæ•´çš„æ–‡ç« æ•°æ®ï¼ŒåŒ…å«å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯
                article_data = {
                    "title": title.strip(),
                    "url": article_url,
                    "content": content,
                    "publish_time": publish_time,
                    "account_name": account_name,
                    "read_count": 0,
                    "like_count": 0,
                    "old_like_count": 0,
                    "share_count": 0,
                    "comment_count": 0,
                    "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

                # ä½¿ç”¨spider_readnum.pyä¸­éªŒè¯æˆåŠŸçš„æ­£åˆ™è¡¨è¾¾å¼æå–ç»Ÿè®¡æ•°æ®

                # æå–é˜…è¯»é‡ - ä½¿ç”¨æˆåŠŸéªŒè¯çš„æ¨¡å¼
                read_num_match = re.search(r"var cgiData = {[^}]*?read_num: '(\d+)'", html_content)
                read_count = int(read_num_match.group(1)) if read_num_match else 0

                if read_count > 0:
                    print(f"ğŸ” é˜…è¯»é‡: {read_count}")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°é˜…è¯»é‡æ•°æ®ï¼Œå¯èƒ½è¯¥æ–‡ç« æœªå…¬å¼€æ˜¾ç¤ºé˜…è¯»é‡")

                article_data["read_count"] = read_count

                # æå–ç‚¹èµæ•° - ä½¿ç”¨æˆåŠŸéªŒè¯çš„æ¨¡å¼
                like_num_match = re.search(r"window\.appmsg_bar_data = {[^}]*?like_count: '(\d+)'", html_content)
                like_count = int(like_num_match.group(1)) if like_num_match else 0

                if like_count > 0:
                    print(f"ğŸ” ç‚¹èµæ•°: {like_count}")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°ç‚¹èµæ•°æ®")

                article_data["like_count"] = like_count

                # æå–å†å²ç‚¹èµæ•° - ä½¿ç”¨æˆåŠŸéªŒè¯çš„æ¨¡å¼
                old_like_num_match = re.search(r"window\.appmsg_bar_data = {[^}]*?old_like_count: '(\d+)'", html_content)
                old_like_count = int(old_like_num_match.group(1)) if old_like_num_match else 0

                if old_like_count > 0:
                    print(f"ğŸ” å†å²ç‚¹èµæ•°: {old_like_count}")

                article_data["old_like_count"] = old_like_count

                # æå–åˆ†äº«æ•° - ä½¿ç”¨æˆåŠŸéªŒè¯çš„æ¨¡å¼
                share_count_match = re.search(r"window\.appmsg_bar_data = {[^}]*?share_count: '(\d+)'", html_content)
                share_count = int(share_count_match.group(1)) if share_count_match else 0

                if share_count > 0:
                    print(f"ğŸ” åˆ†äº«æ•°: {share_count}")

                article_data["share_count"] = share_count

                print(f"âœ… ç»Ÿè®¡æ•°æ®: é˜…è¯»{article_data['read_count']} ç‚¹èµ{article_data['like_count']} åˆ†äº«{article_data['share_count']}")
                return article_data

        except Exception as e:
            print(f"âŒ æå–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def extract_article_content(self, html_content):
        """
        ä»HTMLä¸­æå–æ–‡ç« æ­£æ–‡å†…å®¹
        :param html_content: HTMLå†…å®¹
        :return: æ–‡ç« æ­£æ–‡
        """
        try:
            # æ–¹æ³•1: ä½¿ç”¨BeautifulSoupæ›´å‡†ç¡®åœ°æå–å†…å®¹
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # å°è¯•å¤šç§æ–¹å¼æå–æ–‡ç« å†…å®¹
            content_div = None

            # ä¼˜å…ˆå°è¯•id="js_content"
            content_div = soup.find('div', {'id': 'js_content'})
            if not content_div:
                # å°è¯•class="rich_media_content"
                content_div = soup.find('div', {'class': 'rich_media_content'})
            if not content_div:
                # å°è¯•åŒ…å«rich_media_contentçš„class
                content_div = soup.find('div', class_=lambda x: x and 'rich_media_content' in x)

            if content_div:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_div(["script", "style"]):
                    script.decompose()

                # è·å–çº¯æ–‡æœ¬å†…å®¹
                content_text = content_div.get_text(separator='\n', strip=True)

                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
                content_text = re.sub(r'\n\s*\n', '\n\n', content_text)
                content_text = content_text.strip()

                if content_text:
                    print(f"âœ… æˆåŠŸæå–æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(content_text)} å­—ç¬¦")
                    return content_text

            # æ–¹æ³•2: å¦‚æœBeautifulSoupå¤±è´¥ï¼Œä½¿ç”¨spider_readnum.pyä¸­éªŒè¯æˆåŠŸçš„æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•
            print("ğŸ”„ å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•æå–å†…å®¹...")
            content_match = re.search(r'id="js_content".*?>(.*?)</div>', html_content, re.S)
            if content_match:
                # ç®€å•æ¸…ç†HTMLæ ‡ç­¾
                content = re.sub(r'<.*?>', '', content_match.group(1))
                content = content.strip()
                if content:
                    print(f"âœ… æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                    return content

            print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹")
            return "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"

        except Exception as e:
            print(f"âš ï¸ æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return "æå–å†…å®¹å¤±è´¥"

    def extract_publish_time(self, html_content):
        """
        ä»HTMLä¸­æå–æ–‡ç« å‘å¸ƒæ—¶é—´
        :param html_content: HTMLå†…å®¹
        :return: å‘å¸ƒæ—¶é—´
        """
        try:
            print("ğŸ” å¼€å§‹æå–å‘å¸ƒæ—¶é—´...")

            # ä¼˜å…ˆå°è¯•æå– var createTime = '2025-08-04 14:02'; æ ¼å¼
            createtime_pattern = r"var createTime = '([^']+)'"
            match = re.search(createtime_pattern, html_content)
            if match:
                found_time = match.group(1)
                print(f"âœ… é€šè¿‡createTimeå˜é‡æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {found_time}")
                return found_time

            # å°è¯•å¤šç§æ–¹å¼æå–å‘å¸ƒæ—¶é—´
            time_patterns = [
                # å¸¸è§çš„æ—¥æœŸæ ¼å¼
                (r'<em class="rich_media_meta rich_media_meta_text"[^>]*>(\d{4}-\d{2}-\d{2})</em>', "emæ ‡ç­¾ä¸­çš„æ—¥æœŸ"),
                (r'<span class="rich_media_meta rich_media_meta_text"[^>]*>(\d{4}-\d{2}-\d{2})</span>', "spanæ ‡ç­¾ä¸­çš„æ—¥æœŸ"),
                (r'var publish_time = "(\d{4}-\d{2}-\d{2})"', "JavaScriptå˜é‡ä¸­çš„æ—¥æœŸ"),
                (r'"publish_time":"(\d{4}-\d{2}-\d{2})"', "JSONä¸­çš„æ—¥æœŸ"),

                # æ›´å¤šå¯èƒ½çš„æ ¼å¼
                (r'<em[^>]*class="[^"]*rich_media_meta[^"]*"[^>]*>(\d{4}-\d{2}-\d{2})</em>', "emæ ‡ç­¾å˜ä½“"),
                (r'<span[^>]*class="[^"]*rich_media_meta[^"]*"[^>]*>(\d{4}-\d{2}-\d{2})</span>', "spanæ ‡ç­¾å˜ä½“"),
                (r'publish_time["\']?\s*[:=]\s*["\']?(\d{4}-\d{2}-\d{2})', "é€šç”¨publish_time"),
                (r'createTime["\']?\s*[:=]\s*["\']?(\d{4}-\d{2}-\d{2})', "createTimeå˜é‡"),
                (r'ct\s*=\s*["\']?(\d{10})["\']?', "æ—¶é—´æˆ³æ ¼å¼"),

                # åŒ…å«æ—¶é—´çš„å®Œæ•´æ ¼å¼
                (r'<em class="rich_media_meta rich_media_meta_text"[^>]*>(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})</em>', "å®Œæ•´æ—¶é—´em"),
                (r'<span class="rich_media_meta rich_media_meta_text"[^>]*>(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})</span>', "å®Œæ•´æ—¶é—´span"),

                # ä¸­æ–‡æ ¼å¼
                (r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', "ä¸­æ–‡æ—¥æœŸæ ¼å¼"),
                (r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*(\d{4}-\d{2}-\d{2})', "å‘å¸ƒæ—¶é—´æ ‡ç­¾"),
            ]

            for pattern, description in time_patterns:
                match = re.search(pattern, html_content)
                if match:
                    found_time = match.group(1)
                    print(f"âœ… é€šè¿‡{description}æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {found_time}")

                    # å¦‚æœæ˜¯æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    if pattern.endswith("æ—¶é—´æˆ³æ ¼å¼"):
                        try:
                            timestamp = int(found_time)
                            formatted_time = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
                            print(f"ğŸ”„ æ—¶é—´æˆ³è½¬æ¢ç»“æœ: {formatted_time}")
                            return formatted_time
                        except:
                            pass

                    return found_time

            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æœç´¢ä»»ä½•åŒ…å«æ—¥æœŸçš„æ–‡æœ¬
            print("ğŸ” å°è¯•æœç´¢ä»»ä½•æ—¥æœŸæ ¼å¼...")
            general_date_patterns = [
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{4}/\d{1,2}/\d{1,2})',
                r'(\d{4}\.\d{1,2}\.\d{1,2})',
            ]

            for pattern in general_date_patterns:
                matches = re.findall(pattern, html_content)
                if matches:
                    print(f"ğŸ” æ‰¾åˆ°å¯èƒ½çš„æ—¥æœŸ: {matches[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª

            print("âŒ æœªæ‰¾åˆ°å‘å¸ƒæ—¶é—´")
            return "æœªæ‰¾åˆ°å‘å¸ƒæ—¶é—´"

        except Exception as e:
            print(f"âš ï¸ æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æå–æ—¶é—´å¤±è´¥"

    def extract_account_name(self, html_content):
        """
        ä»HTMLä¸­æå–å…¬ä¼—å·åç§°
        :param html_content: HTMLå†…å®¹
        :return: å…¬ä¼—å·åç§°
        """
        try:
            print("ğŸ” å¼€å§‹æå–å…¬ä¼—å·åç§°...")

            # ä¼˜å…ˆå°è¯•æå– wx_follow_nickname ç±»çš„divä¸­çš„å†…å®¹
            nickname_pattern = r'<div[^>]*class="wx_follow_nickname"[^>]*>\s*([^<]+)\s*</div>'
            match = re.search(nickname_pattern, html_content)
            if match:
                account_name = match.group(1).strip()
                print(f"âœ… é€šè¿‡wx_follow_nicknameæ‰¾åˆ°å…¬ä¼—å·åç§°: {account_name}")
                return account_name

            # å°è¯•å…¶ä»–å¯èƒ½çš„æ¨¡å¼
            name_patterns = [
                # å…¶ä»–å¯èƒ½çš„å…¬ä¼—å·åç§°ä½ç½®
                (r'<span[^>]*class="[^"]*profile_nickname[^"]*"[^>]*>([^<]+)</span>', "profile_nickname"),
                (r'<div[^>]*class="[^"]*account_nickname[^"]*"[^>]*>([^<]+)</div>', "account_nickname"),
                (r'<h1[^>]*class="[^"]*rich_media_title[^"]*"[^>]*>([^<]+)</h1>', "rich_media_title"),
                (r'var nickname = "([^"]+)"', "JavaScriptå˜é‡nickname"),
                (r'"nickname":"([^"]+)"', "JSONä¸­çš„nickname"),
                (r'<meta property="og:site_name" content="([^"]+)"', "og:site_name"),
            ]

            for pattern, description in name_patterns:
                match = re.search(pattern, html_content)
                if match:
                    account_name = match.group(1).strip()
                    print(f"âœ… é€šè¿‡{description}æ‰¾åˆ°å…¬ä¼—å·åç§°: {account_name}")
                    return account_name

            print("âŒ æœªæ‰¾åˆ°å…¬ä¼—å·åç§°")
            return "æœªæ‰¾åˆ°å…¬ä¼—å·åç§°"

        except Exception as e:
            print(f"âš ï¸ æå–å…¬ä¼—å·åç§°å¤±è´¥: {e}")
            return "æå–åç§°å¤±è´¥"

    def clean_html_content(self, html_content):
        """
        æ¸…ç†HTMLå†…å®¹ï¼Œä¿ç•™åŸºæœ¬æ–‡æœ¬
        :param html_content: HTMLå†…å®¹
        :return: æ¸…ç†åçš„æ–‡æœ¬
        """
        try:
            import re

            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
            html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL)

            # ä¿ç•™æ®µè½ç»“æ„ï¼Œå°†pæ ‡ç­¾æ›¿æ¢ä¸ºæ¢è¡Œ
            html_content = re.sub(r'<p[^>]*>', '\n', html_content)
            html_content = re.sub(r'</p>', '\n', html_content)

            # ä¿ç•™æ¢è¡Œæ ‡ç­¾
            html_content = re.sub(r'<br[^>]*>', '\n', html_content)

            # ç§»é™¤å…¶ä»–HTMLæ ‡ç­¾
            html_content = re.sub(r'<[^>]+>', '', html_content)

            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            html_content = re.sub(r'\n\s*\n', '\n\n', html_content)
            html_content = html_content.strip()

            return html_content

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†HTMLå†…å®¹å¤±è´¥: {e}")
            return html_content

    def batch_crawl_readnum(self, max_pages=20, articles_per_page=10, days_back=90):
        """
        æ‰¹é‡æŠ“å–æ–‡ç« é˜…è¯»é‡
        :param max_pages: æœ€å¤§é¡µæ•°
        :param articles_per_page: æ¯é¡µæ–‡ç« æ•°
        :param days_back: æŠ“å–å¤šå°‘å¤©å†…çš„æ–‡ç« 
        :return: æŠ“å–ç»“æœåˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡æŠ“å–é˜…è¯»é‡æ•°æ®")
        print(f"ğŸ“‹ å‚æ•°: æœ€å¤§{max_pages}é¡µï¼Œæ¯é¡µ{articles_per_page}ç¯‡ï¼Œ{days_back}å¤©å†…æ–‡ç« ")

        if not self.load_auth_info():
            print("âŒ è®¤è¯ä¿¡æ¯åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return []

        # éªŒè¯Cookieæœ‰æ•ˆæ€§
        if not self.validate_cookie():
            print("âŒ CookieéªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°è·å–Cookie")
            return []

        all_results = []
        cutoff_date = datetime.now() - timedelta(days=days_back)

        for page in range(max_pages):
            print(f"\n{'='*50}")
            print(f"ğŸ“„ å¤„ç†ç¬¬ {page+1}/{max_pages} é¡µ")

            # è·å–æ–‡ç« åˆ—è¡¨
            articles = self.get_article_list(begin_page=page, count=articles_per_page)

            if not articles:
                print("âŒ æœªè·å–åˆ°æ–‡ç« ï¼Œåœæ­¢æŠ“å–")
                break

            page_results = []
            outdated_count = 0

            for i, article in enumerate(articles):
                print(f"\nğŸ“– å¤„ç†æ–‡ç«  {i+1}/{len(articles)}: {article['title'][:30]}...")

                # æ£€æŸ¥æ–‡ç« æ—¶é—´
                if article['create_time']:
                    try:
                        article_date = datetime.fromtimestamp(article['create_time'])
                        if article_date < cutoff_date:
                            print(f"â° æ–‡ç« è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œè·³è¿‡")
                            outdated_count += 1
                            continue
                    except:
                        pass

                # æŠ“å–æ–‡ç« å†…å®¹å’Œç»Ÿè®¡æ•°æ®
                article_data = self.extract_article_content_and_stats(article['url'])

                if article_data:
                    # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç 
                    if article_data.get('error') == 'captcha_required':
                        print(f"ğŸ›‘ é‡åˆ°éªŒè¯ç ï¼Œåœæ­¢æ‰¹é‡æŠ“å–")
                        print(f"ğŸ’¡ å»ºè®®ï¼šæ‰‹åŠ¨å®ŒæˆéªŒè¯åé‡æ–°è¿è¡Œï¼Œæˆ–é™ä½æŠ“å–é¢‘ç‡")
                        break

                    # æ£€æŸ¥æ˜¯å¦ä¸ºéæ–‡ç« é¡µé¢
                    elif article_data.get('error') == 'not_article_page':
                        print(f"âš ï¸ éæ–‡ç« é¡µé¢ï¼Œè·³è¿‡")
                        continue

                    # æ­£å¸¸çš„ç»Ÿè®¡æ•°æ®
                    else:
                        # åˆå¹¶æ–‡ç« ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®
                        result = {
                            **article,
                            **article_data,
                            "pub_time": datetime.fromtimestamp(article['create_time']).strftime("%Y-%m-%d %H:%M:%S") if article['create_time'] else ""
                        }

                        # å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“
                        if self.save_to_db and self.db_manager:
                            try:
                                # å‡†å¤‡æ•°æ®åº“æ’å…¥æ•°æ®
                                db_article_data = {
                                    'title': result.get('title', ''),
                                    'content': result.get('content', ''),
                                    'url': result.get('url', ''),
                                    'pub_time': result.get('pub_time', ''),
                                    'crawl_time': result.get('crawl_time', ''),
                                    'unit_name': self.unit_name or result.get('account_name', ''),
                                    'view_count': result.get('read_count', 0),
                                    'like_count': result.get('like_count', 0),
                                    'share_count': result.get('share_count', 0)
                                }

                                success = self.db_manager.insert_article(db_article_data)
                                if success:
                                    print(f"ğŸ’¾ ç¬¬{len(all_results)+1}ç¯‡æ–‡ç« å·²ä¿å­˜åˆ°æ•°æ®åº“: {result.get('title', 'Unknown')}")
                                else:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ ‡é¢˜é‡å¤è€Œè·³è¿‡
                                    if result.get('title', '').strip() and self.db_manager.check_article_title_exists(result.get('title', '').strip()):
                                        print(f"âš ï¸ ç¬¬{len(all_results)+1}ç¯‡æ–‡ç« æ ‡é¢˜é‡å¤ï¼Œå·²è·³è¿‡: {result.get('title', 'Unknown')}")
                                    else:
                                        print(f"âŒ ç¬¬{len(all_results)+1}ç¯‡æ–‡ç« æ•°æ®åº“ä¿å­˜å¤±è´¥: {result.get('title', 'Unknown')}")
                            except Exception as e:
                                print(f"âŒ æ•°æ®åº“ä¿å­˜å‡ºé”™: {e}")

                        page_results.append(result)
                        all_results.append(result)

                        print(f"âœ… å®Œæˆ {len(all_results)} ç¯‡æ–‡ç« ")
                else:
                    print(f"âŒ ç»Ÿè®¡æ•°æ®è·å–å¤±è´¥")

                # æ–‡ç« é—´å»¶è¿Ÿ
                if i < len(articles) - 1:
                    delay = random.randint(10, 15)
                    print(f"â³ æ–‡ç« é—´å»¶è¿Ÿ {delay} ç§’...")
                    time.sleep(delay)

            print(f"ğŸ“Š æœ¬é¡µå®Œæˆ {len(page_results)} ç¯‡æ–‡ç« ï¼Œè¶…æ—¶ {outdated_count} ç¯‡")

            # å¦‚æœæœ¬é¡µå¤§éƒ¨åˆ†æ–‡ç« éƒ½è¶…æ—¶ï¼Œåœæ­¢æŠ“å–
            if outdated_count > len(articles) * 0.7:
                print("ğŸ›‘ å¤§éƒ¨åˆ†æ–‡ç« è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œåœæ­¢æŠ“å–")
                break

            # é¡µé¢é—´å»¶è¿Ÿ
            if page < max_pages - 1:
                page_delay = random.randint(10, 20)
                print(f"â³ é¡µé¢é—´å»¶è¿Ÿ {page_delay} ç§’...")
                time.sleep(page_delay)

        self.articles_data = all_results

        # å…³é—­æ•°æ®åº“è¿æ¥
        if self.db_manager:
            self.db_manager.disconnect()
            print("ğŸ’¾ æ•°æ®åº“è¿æ¥å·²å…³é—­")

        print(f"\nğŸ‰ æ‰¹é‡æŠ“å–å®Œæˆï¼å…±è·å– {len(all_results)} ç¯‡æ–‡ç« çš„ç»Ÿè®¡æ•°æ®")
        if self.save_to_db:
            print(f"ğŸ’¾ æ•°æ®å·²å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“")

        return all_results

    def save_to_excel(self, filename=None):
        """
        ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶
        :param filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        :return: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.articles_data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"./data/readnum_batch/readnum_batch_{timestamp}.xlsx"

        try:
            # å‡†å¤‡Excelæ•°æ® - åŒ…å«å‘å¸ƒæ—¶é—´å’Œå…¬ä¼—å·åç§°
            excel_data = []
            for article in self.articles_data:
                excel_data.append({
                    'æ ‡é¢˜': article.get('title', ''),
                    'å…¬ä¼—å·åç§°': article.get('account_name', ''),
                    'å‘å¸ƒæ—¶é—´': article.get('publish_time', '') or article.get('pub_time', ''),
                    'é˜…è¯»é‡': article.get('read_count', 0),
                    'ç‚¹èµæ•°': article.get('like_count', 0),
                    'å†å²ç‚¹èµæ•°': article.get('old_like_count', 0),
                    'åˆ†äº«æ•°': article.get('share_count', 0),
                    'è¯„è®ºæ•°': article.get('comment_count', 0),
                    'æ–‡ç« é“¾æ¥': article.get('url', ''),
                    'æ‘˜è¦': article.get('digest', ''),
                    'æŠ“å–æ—¶é—´': article.get('crawl_time', '')
                })

            # åˆ›å»ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(excel_data)

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filename), exist_ok=True)

            # ä¿å­˜åˆ°Excel
            df.to_excel(filename, index=False, engine='openpyxl')

            print(f"ğŸ“Š Excelæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            print(f"ğŸ“ˆ å…±ä¿å­˜ {len(excel_data)} æ¡è®°å½•")

            return filename

        except Exception as e:
            print(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_to_json(self, filename=None):
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        :param filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        :return: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.articles_data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"./data/readnum_batch/readnum_batch_{timestamp}.json"

        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filename), exist_ok=True)

            # å‡†å¤‡JSONæ•°æ® - å»æ‰ä½œè€…å­—æ®µ
            json_data = []
            for article in self.articles_data:
                # å¤åˆ¶æ–‡ç« æ•°æ®ä½†å»æ‰ä½œè€…å­—æ®µ
                clean_article = {k: v for k, v in article.items() if k != 'author'}
                json_data.append(clean_article)

            # ä¿å­˜åˆ°JSON
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            print(f"ğŸ“ˆ å…±ä¿å­˜ {len(self.articles_data)} æ¡è®°å½•")

            return filename

        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def generate_summary_report(self):
        """
        ç”Ÿæˆç»Ÿè®¡æ‘˜è¦æŠ¥å‘Š
        :return: æ‘˜è¦ä¿¡æ¯å­—å…¸
        """
        if not self.articles_data:
            return None

        total_articles = len(self.articles_data)
        total_reads = sum(article.get('read_count', 0) for article in self.articles_data)
        total_likes = sum(article.get('like_count', 0) for article in self.articles_data)
        total_shares = sum(article.get('share_count', 0) for article in self.articles_data)

        avg_reads = total_reads / total_articles if total_articles > 0 else 0
        avg_likes = total_likes / total_articles if total_articles > 0 else 0
        avg_shares = total_shares / total_articles if total_articles > 0 else 0

        # æ‰¾å‡ºé˜…è¯»é‡æœ€é«˜çš„æ–‡ç« 
        top_article = max(self.articles_data, key=lambda x: x.get('read_count', 0))

        summary = {
            "total_articles": total_articles,
            "total_reads": total_reads,
            "total_likes": total_likes,
            "total_shares": total_shares,
            "avg_reads": round(avg_reads, 2),
            "avg_likes": round(avg_likes, 2),
            "avg_shares": round(avg_shares, 2),
            "top_article": {
                "title": top_article.get('title', ''),
                "read_count": top_article.get('read_count', 0),
                "like_count": top_article.get('like_count', 0),
                "url": top_article.get('url', '')
            },
            "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        return summary

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        summary = self.generate_summary_report()
        if not summary:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ç»Ÿè®¡")
            return

        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ‰¹é‡é˜…è¯»é‡æŠ“å–ç»Ÿè®¡æ‘˜è¦")
        print(f"{'='*60}")
        print(f"ğŸ“– æ€»æ–‡ç« æ•°: {summary['total_articles']}")
        print(f"ğŸ‘€ æ€»é˜…è¯»é‡: {summary['total_reads']:,}")
        print(f"ğŸ‘ æ€»ç‚¹èµæ•°: {summary['total_likes']:,}")
        print(f"ğŸ“¤ æ€»åˆ†äº«æ•°: {summary['total_shares']:,}")
        print(f"ğŸ“Š å¹³å‡é˜…è¯»é‡: {summary['avg_reads']:,.2f}")
        print(f"ğŸ“Š å¹³å‡ç‚¹èµæ•°: {summary['avg_likes']:.2f}")
        print(f"ğŸ“Š å¹³å‡åˆ†äº«æ•°: {summary['avg_shares']:.2f}")
        print(f"\nğŸ† é˜…è¯»é‡æœ€é«˜æ–‡ç« :")
        print(f"   æ ‡é¢˜: {summary['top_article']['title']}")
        print(f"   é˜…è¯»é‡: {summary['top_article']['read_count']:,}")
        print(f"   ç‚¹èµæ•°: {summary['top_article']['like_count']:,}")
        print(f"\nâ° ç»Ÿè®¡æ—¶é—´: {summary['crawl_time']}")
        print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸš€ å¾®ä¿¡å…¬ä¼—å·æ‰¹é‡é˜…è¯»é‡æŠ“å–å™¨")
    print("="*50)

    # åˆå§‹åŒ–çˆ¬è™«
    spider = BatchReadnumSpider()

    try:
        # æ‰¹é‡æŠ“å–é˜…è¯»é‡ï¼ˆæœ€è¿‘7å¤©ï¼Œæœ€å¤š3é¡µï¼Œæ¯é¡µ5ç¯‡ï¼‰
        results = spider.batch_crawl_readnum(max_pages=3, articles_per_page=5, days_back=1)

        if results:
            # æ‰“å°ç»Ÿè®¡æ‘˜è¦
            spider.print_summary()

            # ä¿å­˜æ•°æ®
            excel_file = spider.save_to_excel()
            json_file = spider.save_to_json()

            print(f"\nâœ… æŠ“å–å®Œæˆï¼")
            if excel_file:
                print(f"ğŸ“Š Excelæ–‡ä»¶: {excel_file}")
            if json_file:
                print(f"ğŸ’¾ JSONæ–‡ä»¶: {json_file}")
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
